{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/younesabdolmalaky/LTR-on-torob-data/blob/main/notebooks/RankNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tcca3MtSJF3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c506bdf-0cac-44ac-f383-47ad36bf5e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8sPAVZnJJD0K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gc\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import sparse\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input , Dense , Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9BpVvIiWUFax"
      },
      "outputs": [],
      "source": [
        "def read_json_lines(path, n_lines=None):\n",
        "    \"\"\"Creates a generator which reads and returns lines of\n",
        "    a json lines file, one line at a time, each as a dictionary.\n",
        "    \n",
        "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
        "    for reading a json lines file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines == i:\n",
        "                break\n",
        "            yield json.loads(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UnflyiLQSnZd"
      },
      "outputs": [],
      "source": [
        "aggregated_search_data_path = '/content/drive/MyDrive/torob/output_data/aggregated_search_data.jsonl'\n",
        "preprocessed_products_path = '/content/drive/MyDrive/torob/output_data/preprocessed_products.jsonl'\n",
        "preprocessed_test_queries_path = '/content/drive/MyDrive/torob/output_data/preprocessed_test_queries.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZnPiWnWIUB2R"
      },
      "outputs": [],
      "source": [
        "aggregated_searches_df = pd.DataFrame(read_json_lines(aggregated_search_data_path, n_lines=None))\n",
        "products_data_df = pd.DataFrame(read_json_lines(preprocessed_products_path))\n",
        "test_offline_queries_df = pd.DataFrame(read_json_lines(preprocessed_test_queries_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DekT-dzjUH66"
      },
      "outputs": [],
      "source": [
        "products_id_to_idx = dict(\n",
        "    (p_id, idx)\n",
        "    for idx, p_id in enumerate(products_data_df['id'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PwT8HpofJJT2"
      },
      "outputs": [],
      "source": [
        "def getDatasetSize(aggregated_searches_df , n_candidates = None):\n",
        "  counter = 0\n",
        "  for qid, agg_search in(enumerate(aggregated_searches_df.itertuples(index=False))):\n",
        "\n",
        "    if n_candidates is None:\n",
        "        limit = len(agg_search.results)\n",
        "    else:\n",
        "        limit = min(n_candidates, len(agg_search.results))\n",
        "    clicks = dict(zip(agg_search.clicks, agg_search.clicks_count))\n",
        "\n",
        "    for i, candidate_product_id in enumerate(agg_search.results[:limit]):\n",
        "        if candidate_product_id is None:\n",
        "            continue\n",
        "\n",
        "        counter = counter + 1\n",
        "  return counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "j2T_x-Y8I8OE"
      },
      "outputs": [],
      "source": [
        "class DualChannelDataGenerator(Sequence):\n",
        "    def __init__(self, dataset_size , query, doc , aggregated_searches_df , vectorsize , batch_size):\n",
        "      self.dataset_size = dataset_size\n",
        "      self.query = query\n",
        "      self.doc = doc\n",
        "      self.batch_size = batch_size\n",
        "      self.aggregated_searches_df = aggregated_searches_df\n",
        "      self.loop1 = 0\n",
        "      self.loop2 = 0\n",
        "      self.vectorsize = vectorsize\n",
        "      self.n_candidates = None\n",
        "      self.random_projection_mat = np.random.rand(8196, 256)\n",
        "      np.save('/content/drive/MyDrive/torob/Features/rand.npy', self.random_projection_mat , allow_pickle=True)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.dataset_size/ float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self , idx):\n",
        "      a=np.zeros((self.batch_size, self.vectorsize) , dtype = float)\n",
        "      b=np.zeros((self.batch_size, self.vectorsize) , dtype = float)\n",
        "      batch_y = np.ones((self.batch_size))\n",
        "\n",
        "      counter = 0\n",
        "      loop = False\n",
        "      loop11 = self.loop1\n",
        "      loop22 = self.loop2\n",
        "      for qid, agg_search in (enumerate(aggregated_searches_df[self.loop1:].itertuples(index=False))):\n",
        "        if loop == True:\n",
        "          break\n",
        "\n",
        "        if self.n_candidates is None:\n",
        "            limit = len(agg_search.results)\n",
        "        else:\n",
        "            limit = min(self.n_candidates, len(agg_search.results))\n",
        "        clicks = dict(zip(agg_search.clicks, agg_search.clicks_count))\n",
        "\n",
        "        for candidate_product_id in agg_search.results[:limit][self.loop2:]:\n",
        "\n",
        "          if candidate_product_id is None:\n",
        "                continue\n",
        "\n",
        "          candidate_score = clicks.get(candidate_product_id, 0)\n",
        "          candidate_score = np.log2(candidate_score + 1)\n",
        "\n",
        "          loop22 = loop22 + 1\n",
        "          counter = counter + 1\n",
        "          if counter >= self.batch_size:\n",
        "            loop = True\n",
        "            self.loop2 = loop22\n",
        "            self.loop1 = loop11\n",
        "            break\n",
        "\n",
        "          p_idx = products_id_to_idx[candidate_product_id]\n",
        "          a[counter]=(query[qid]).toarray()\n",
        "          b[counter]=(doc[p_idx]).toarray()\n",
        "          batch_y[counter] = candidate_score \n",
        "\n",
        "\n",
        "        if loop == False:\n",
        "          loop22 = 0\n",
        "          loop11 = loop11 + 1\n",
        "      return np.hstack((a.dot(self.random_projection_mat),b.dot(self.random_projection_mat))) , batch_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "StLRZD5DJNjJ"
      },
      "outputs": [],
      "source": [
        "input = tf.keras.layers.Input(shape=(512,), name='input')\n",
        "x1 = tf.keras.layers.Dense(1024, activation='relu')(input)\n",
        "x1 = tf.keras.layers.Dense(2048, activation='relu')(x1)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(x1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oG29UW8qJSvd"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Model(inputs=input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "OcW6ip03JTxi"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(filepath='/content/drive/MyDrive/model_{epoch}.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UVHzP3PEdJn1"
      },
      "outputs": [],
      "source": [
        "def ranknet_loss(y_true, y_pred):\n",
        "    y_true = 2 * y_true - 1\n",
        "\n",
        "    pairwise_diff = tf.expand_dims(y_true, axis=1) - tf.expand_dims(y_true, axis=0)\n",
        "\n",
        "    pairwise_logits = tf.expand_dims(y_pred, axis=1) - tf.expand_dims(y_pred, axis=0)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(pairwise_diff, 0), tf.float32)\n",
        "    pairwise_logits = pairwise_logits * mask\n",
        "\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.sigmoid(pairwise_diff), logits=pairwise_logits))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kSkvYC49JbKZ"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=ranknet_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yzWPRQZAXuSS"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/torob/Features/doc_tfidf.h5', 'rb') as f:\n",
        "    doc = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/torob/Features/train_query_tfidf.h5', 'rb') as f:\n",
        "    query = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = getDatasetSize(aggregated_searches_df)\n",
        "vectorsize = doc.shape[1]\n",
        "batch_size = 8192"
      ],
      "metadata": {
        "id": "AdFqlCP4rJ2t"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5bg7Eu6dJfG7"
      },
      "outputs": [],
      "source": [
        "train_generator = DualChannelDataGenerator( \n",
        "      dataset_size ,\n",
        "      query,\n",
        "      doc, \n",
        "      aggregated_searches_df , \n",
        "      vectorsize ,\n",
        "      batch_size\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwkhRUWiJgKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8538d4-33de-44bc-8d1f-8862e23c8168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2930/2930 [==============================] - 12843s 4s/step - loss: 0.6934\n",
            "Epoch 2/5\n",
            "2930/2930 [==============================] - 12428s 4s/step - loss: 0.6931\n",
            "Epoch 3/5\n",
            "  34/2930 [..............................] - ETA: 3:24:25 - loss: 0.6931"
          ]
        }
      ],
      "source": [
        "model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=5,callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgMwjG46K7u8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuaGb3pJXN9nzbNvHbiDTD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}