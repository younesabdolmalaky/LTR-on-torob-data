{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuNP8imS5PSYZGvM+U7f+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/younesabdolmalaky/LTR-on-torob-data/blob/main/notebooks/FeatureExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCrdKfQzO9Jz",
        "outputId": "cee4a304-ea68-42fb-d1b9-ba5052597709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence"
      ],
      "metadata": {
        "id": "euDi9KY3PAwd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json_lines(path, n_lines=None):\n",
        "    \"\"\"Creates a generator which reads and returns lines of\n",
        "    a json lines file, one line at a time, each as a dictionary.\n",
        "    \n",
        "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
        "    for reading a json lines file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines == i:\n",
        "                break\n",
        "            yield json.loads(line)\n",
        "            \n",
        "aggregated_search_data_path = '/content/drive/MyDrive/torob/output_data/aggregated_search_data.jsonl'\n",
        "preprocessed_products_path = '/content/drive/MyDrive/torob/output_data/preprocessed_products.jsonl'\n",
        "preprocessed_test_queries_path = '/content/drive/MyDrive/torob/output_data/preprocessed_test_queries.jsonl'"
      ],
      "metadata": {
        "id": "2uQFF5x-PD7r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_data_df = pd.DataFrame(read_json_lines(preprocessed_products_path))"
      ],
      "metadata": {
        "id": "q2zV3-sjPGa4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_FEATURES = 12000\n",
        "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
        "tokenizer.fit_on_texts(products_data_df['title_normalized'])\n",
        "doc = tokenizer.texts_to_sequences(products_data_df['title_normalized'])\n",
        "del products_data_df"
      ],
      "metadata": {
        "id": "9F1icDF4XEQx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = max(len(train_ex) for train_ex in doc)"
      ],
      "metadata": {
        "id": "eFqGXaJYXM4i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = pad_sequences(doc, maxlen=MAX_LENGTH)\n",
        "pickle.dump(doc, open(\"/content/drive/MyDrive/torob/Features/doc_sequences.h5\", \"wb\"))\n",
        "del doc"
      ],
      "metadata": {
        "id": "T8CpKcFXXPbL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated_searches_df = pd.DataFrame(read_json_lines(aggregated_search_data_path, n_lines=None))\n",
        "train_query = tokenizer.texts_to_sequences(aggregated_searches_df['raw_query_normalized'])\n",
        "train_query = pad_sequences(train_query, maxlen=MAX_LENGTH)\n",
        "pickle.dump(train_query, open(\"/content/drive/MyDrive/torob/Features/train_query_sequences.h5\", \"wb\"))\n",
        "del train_query\n",
        "del aggregated_searches_df "
      ],
      "metadata": {
        "id": "KfGjpXOaV-Jf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_offline_queries_df = pd.DataFrame(read_json_lines(preprocessed_test_queries_path))\n",
        "test_query =  tokenizer.texts_to_sequences(test_offline_queries_df['raw_query_normalized'])\n",
        "test_query = pad_sequences(test_query, maxlen=MAX_LENGTH)\n",
        "pickle.dump(test_query, open(\"/content/drive/MyDrive/torob/Features/test_query_sequences.h5\", \"wb\"))\n",
        "del test_query\n",
        "del test_offline_queries_df"
      ],
      "metadata": {
        "id": "JlrZiMBuYO25"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "metadata": {
        "id": "46rS9brXT-at"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "Q1b9yzqWY5s3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json_lines(path, n_lines=None):\n",
        "    \"\"\"Creates a generator which reads and returns lines of\n",
        "    a json lines file, one line at a time, each as a dictionary.\n",
        "    \n",
        "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
        "    for reading a json lines file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines == i:\n",
        "                break\n",
        "            yield json.loads(line)\n",
        "            \n",
        "aggregated_search_data_path = '/content/drive/MyDrive/torob/output_data/aggregated_search_data.jsonl'\n",
        "preprocessed_products_path = '/content/drive/MyDrive/torob/output_data/preprocessed_products.jsonl'\n",
        "preprocessed_test_queries_path = '/content/drive/MyDrive/torob/output_data/preprocessed_test_queries.jsonl'"
      ],
      "metadata": {
        "id": "PNO-khwtZD_D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 8196\n",
        "EMBEDDING_DIM = 256\n",
        "NUM_TRAIN_SAMPLES = None"
      ],
      "metadata": {
        "id": "SerCJt0ePM9c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=VOCAB_SIZE, lowercase=True, use_idf=True)\n",
        "products_data_df = pd.DataFrame(read_json_lines(preprocessed_products_path))\n",
        "products_tfidf = vectorizer.fit_transform(products_data_df['title_normalized'])\n",
        "del products_data_df\n",
        "pickle.dump(products_tfidf, open(\"/content/drive/MyDrive/torob/Features/doc_tfidf.h5\", \"wb\"))\n",
        "del products_tfidf"
      ],
      "metadata": {
        "id": "3-poIifiPY1u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated_searches_df = pd.DataFrame(read_json_lines(aggregated_search_data_path, n_lines=None))\n",
        "queries_train_tfidf = vectorizer.transform(aggregated_searches_df['raw_query_normalized'])\n",
        "pickle.dump(queries_train_tfidf, open(\"/content/drive/MyDrive/torob/Features/train_query_tfidf.h5\", \"wb\"))\n",
        "del queries_train_tfidf\n",
        "del aggregated_searches_df"
      ],
      "metadata": {
        "id": "T_Gd7UhjPQAb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_offline_queries_df = pd.DataFrame(read_json_lines(preprocessed_test_queries_path))\n",
        "queries_test_tfidf = vectorizer.transform(test_offline_queries_df['raw_query_normalized'])\n",
        "pickle.dump(queries_test_tfidf, open(\"/content/drive/MyDrive/torob/Features/test_query_tfidf.h5\", \"wb\"))\n",
        "del queries_test_tfidf\n",
        "del test_offline_queries_df"
      ],
      "metadata": {
        "id": "vHQVXHC9PSj-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6vh7jfITzx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}